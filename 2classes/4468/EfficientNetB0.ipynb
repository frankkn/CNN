{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "from dataclasses import dataclass\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils, datasets, layers, models\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4fdc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConstParams:\n",
    "    MODEL_NAME = \"EfficientNetB0\"\n",
    "\n",
    "@dataclass\n",
    "class HParams:\n",
    "    IMAGE_HEIGHT = 224\n",
    "    IMAGE_WIDTH = 224\n",
    "    IMAGE_DEPTH = 3\n",
    "    \n",
    "    BUFFER_SIZE = 100\n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    SEED = 42\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    NUM_EPOCH = 50\n",
    "    LEARNING_RATE = 1e-5\n",
    "\n",
    "@dataclass\n",
    "class Paths:\n",
    "    DATA_ROOT: Path = Path(\"./dataset\")\n",
    "    CHECKPOINT_DIR: Path = Path(\"./checkpoint/\" + ConstParams.MODEL_NAME)\n",
    "    BEST_CHECKPOINT_DIR: Path = Path(\"./checkpoint/\" + ConstParams.MODEL_NAME + \"/best\")\n",
    "    # save (img_path, label) pairs\n",
    "    TRAIN_CSV_PATH: Path = Path(\"./csv/train.csv\")\n",
    "    VAL_CSV_PATH: Path = Path(\"./csv/val.csv\")\n",
    "    TEST_CSV_PATH: Path = Path(\"./csv/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd231e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(HParams.SEED)\n",
    "np.random.seed(HParams.SEED)\n",
    "random.seed(HParams.SEED)\n",
    "\n",
    "if not Path(\"./dataset\").exists():\n",
    "    Path.mkdir(Path(\"./dataset\"))\n",
    "\n",
    "if not Path(\"./checkpoint\").exists():\n",
    "    Path.mkdir(Path(\"./checkpoint\"))\n",
    "\n",
    "if not Path(\"./csv\").exists():\n",
    "    Path.mkdir(Path(\"./csv\"))\n",
    "\n",
    "if not Paths.CHECKPOINT_DIR.exists():\n",
    "    Path.mkdir(Paths.CHECKPOINT_DIR)\n",
    "\n",
    "if not Paths.BEST_CHECKPOINT_DIR.exists():\n",
    "    Path.mkdir(Paths.BEST_CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b96740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the subfolders.\n",
    "print('classes:')\n",
    "for item in Paths.DATA_ROOT.iterdir():\n",
    "    print(item)\n",
    "    \n",
    "all_image_paths = list(Paths.DATA_ROOT.glob('*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "all_image_paths = shuffle(all_image_paths, random_state=HParams.RANDOM_STATE)\n",
    "# all_image_paths = [path for path in all_image_paths if path[-3:] not in ('gif', 'bmp', 'ini')]\n",
    "all_image_paths = [path for path in all_image_paths if os.path.splitext(path)[1][1:] in ('jpg', 'JPG')]\n",
    "\n",
    "image_count = len(all_image_paths)\n",
    "print('\\ntotal img num:', image_count)\n",
    "# print(f'all image path = {all_image_paths}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random showing 3 iamges\n",
    "for n in range(3):\n",
    "    image_path = random.choice(all_image_paths)\n",
    "    display.display(display.Image(image_path, width=200, height=200))\n",
    "    print(image_path.split('\\\\')[-2])\n",
    "    # print(image_path.split(os.path.sep)[-2])\n",
    "    # print(image_path.split('/')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a817f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the label\n",
    "label_names = sorted(item.name for item in Paths.DATA_ROOT.glob('*/') if item.is_dir())\n",
    "# total label\n",
    "n_classes = len(label_names)\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mapping dict\n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "index_to_label = dict((index, name) for index,name in enumerate(label_names))\n",
    "print(label_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the label data\n",
    "all_image_label = [label_to_index[Path(path).parent.name] for path in all_image_paths]\n",
    "print(\"First 10 label indices: \", all_image_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3832732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train:val:test datasets using an 80-10-10 split\n",
    "img_path_train, img_path_val_and_test, label_train, label_val_and_test = train_test_split(all_image_paths, \n",
    "                                                                                          all_image_label,\n",
    "                                                                                          test_size=0.2,\n",
    "                                                                                          random_state=HParams.RANDOM_STATE)\n",
    "img_path_val, img_path_test, label_val, label_test = train_test_split(img_path_val_and_test, \n",
    "                                                                      label_val_and_test, \n",
    "                                                                      test_size=0.5, \n",
    "                                                                      random_state=HParams.RANDOM_STATE)\n",
    "\n",
    "\n",
    "print('training data: %d'%(len(img_path_train)))\n",
    "print('validation data: %d'%(len(img_path_val)))\n",
    "print('testing data: %d'%(len(img_path_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_ready = False\n",
    "\n",
    "if pairs_ready != True:\n",
    "    # save (img_path, label) pairs\n",
    "    with open(Paths.TRAIN_CSV_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['img_path', 'label'])\n",
    "        for img_path, label in zip(img_path_train, label_train):\n",
    "            writer.writerow([img_path, label])\n",
    "\n",
    "    with open(Paths.VAL_CSV_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['img_path', 'label'])\n",
    "        for img_path, label in zip(img_path_val, label_val):\n",
    "            writer.writerow([img_path, label])\n",
    "\n",
    "    with open(Paths.TEST_CSV_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['img_path', 'label'])\n",
    "        for img_path, label in zip(img_path_test, label_test):\n",
    "            writer.writerow([img_path, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(tf.data.Dataset):\n",
    "    OUTPUT_SIGNATURE = (\n",
    "        tf.TensorSpec(shape=(HParams.IMAGE_HEIGHT, HParams.IMAGE_WIDTH, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    "\n",
    "    _INSTANCES_COUNTER = itertools.count()  # Number of datasets generated\n",
    "    _EPOCHS_COUNTER = defaultdict(itertools.count)  # Number of epochs done for each dataset\n",
    "\n",
    "    def _generator(instance_idx, filename, open_file, read_file):\n",
    "        epoch_idx = next(SimpleDataset._EPOCHS_COUNTER[instance_idx])\n",
    "\n",
    "        # Opening the file\n",
    "        img_paths, label = open_file(filename)\n",
    "\n",
    "        # Reading the file\n",
    "        for sample_idx in range(len(img_paths)):\n",
    "            img = read_file(img_paths[sample_idx])\n",
    "            yield img, label[sample_idx]\n",
    "\n",
    "    def __new__(cls, filename, open_file, read_file):\n",
    "        def generator_func(instance_idx, filename):\n",
    "            return cls._generator(instance_idx, filename, open_file, read_file)\n",
    "\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            generator_func,\n",
    "            output_signature=cls.OUTPUT_SIGNATURE,\n",
    "            args=(next(cls._INSTANCES_COUNTER), filename)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    rows = pd.read_csv(filename.decode(\"utf-8\"))\n",
    "    img_paths = rows['img_path'].tolist()\n",
    "    label = rows['label'].tolist()\n",
    "    return img_paths, label\n",
    "\n",
    "def read_file(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=HParams.IMAGE_DEPTH)\n",
    "    img = tf.image.resize(img, (HParams.IMAGE_HEIGHT, HParams.IMAGE_WIDTH))\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = tf.divide(img,255.0)\n",
    "    return img\n",
    "\n",
    "def dataset_generator_fun_train(*args):\n",
    "    return SimpleDataset(str(Paths.TRAIN_CSV_PATH), open_file, read_file)\n",
    "\n",
    "def dataset_generator_fun_val(*args):\n",
    "    return SimpleDataset(str(Paths.VAL_CSV_PATH), open_file, read_file)\n",
    "\n",
    "def dataset_generator_fun_test(*args):\n",
    "    return SimpleDataset(str(Paths.TEST_CSV_PATH), open_file, read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse training data\n",
    "@tf.function\n",
    "def map_fun(image, label):\n",
    "    distorted_image = tf.image.random_flip_left_right(image)\n",
    "    distorted_image = tf.image.random_brightness(distorted_image, max_delta=60)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image, lower=0.2, upper=1.8)\n",
    "    distorted_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "    return distorted_image, label\n",
    "\n",
    "# parse validation data\n",
    "@tf.function\n",
    "def map_fun_val(image, label):\n",
    "    distorted_image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    return distorted_image, label\n",
    "\n",
    "# parse testing data\n",
    "@tf.function\n",
    "def map_fun_test(image, label):\n",
    "    distorted_image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    return distorted_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.range(1)\\\n",
    "                                .interleave(dataset_generator_fun_train, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                .shuffle(HParams.BUFFER_SIZE, seed=HParams.SEED)\\\n",
    "                                .batch(HParams.BATCH_SIZE, drop_remainder=True)\\\n",
    "                                .map(map_fun, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                .cache()\\\n",
    "                                .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "dataset_val = tf.data.Dataset.range(1)\\\n",
    "                                .interleave(dataset_generator_fun_val, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                .batch(HParams.BATCH_SIZE, drop_remainder=True)\\\n",
    "                                .shuffle(HParams.BUFFER_SIZE, seed=HParams.SEED)\\\n",
    "                                .map(map_fun_val, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                .cache()\\\n",
    "                                .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "dataset_test = tf.data.Dataset.range(1)\\\n",
    "                                .interleave(dataset_generator_fun_test, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                .batch(HParams.BATCH_SIZE, drop_remainder=True)\\\n",
    "                                .map(map_fun_test, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                                .cache()\\\n",
    "                                .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in dataset_train.take(1):\n",
    "    print(img[0].shape)\n",
    "    plt.imshow(img[0]) \n",
    "    plt.axis('off') \n",
    "    plt.show()\n",
    "    print(index_to_label[label[0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB0(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=(HParams.IMAGE_HEIGHT, HParams.IMAGE_WIDTH, HParams.IMAGE_DEPTH),\n",
    "                   pooling=None\n",
    "                  )\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "top_model = models.Sequential()\n",
    "top_model.add(layers.Flatten())\n",
    "top_model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=top_model(base_model.output)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0a31b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(HParams.LEARNING_RATE)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(image, training=True)\n",
    "        loss = loss_object(label, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "#     print(\"Label shape:\", label.shape)\n",
    "#     print(\"Predictions shape:\", predictions.shape)\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(label, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def val_step(image, label):\n",
    "    predictions = model(image, training=False)\n",
    "    loss = loss_object(label, predictions)\n",
    "\n",
    "    val_loss(loss)\n",
    "    val_accuracy(label, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(image, label):\n",
    "    predictions = model(image, training=False)\n",
    "    loss = loss_object(label, predictions)\n",
    "\n",
    "    return loss, label, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ee625",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    checkpoint, Paths.CHECKPOINT_DIR, max_to_keep=3\n",
    ")\n",
    "best_manager = tf.train.CheckpointManager(\n",
    "    checkpoint, Paths.BEST_CHECKPOINT_DIR, max_to_keep=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b8ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def epoch_loop(dataset_train, dataset_test, EPOCHS):\n",
    "    best_val_acc = 0.\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"=\" * 10 + f\" Epoch {epoch + 1}/{EPOCHS} \" + \"=\" * 10)\n",
    "        # Reset the metrics at the start of the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "\n",
    "        tf.print(\"training:\")\n",
    "        for image, label in tqdm(dataset_train, total=math.floor(len(img_path_train)/HParams.BATCH_SIZE)):\n",
    "            train_step(image, label)\n",
    "\n",
    "        tf.print(\"validating:\")\n",
    "        for image, label in tqdm(dataset_val, total=math.floor(len(img_path_val)/HParams.BATCH_SIZE)):\n",
    "            val_step(image, label)\n",
    "            \n",
    "        train_loss_list.append(train_loss.result().numpy())\n",
    "        val_loss_list.append(val_loss.result().numpy())\n",
    "     \n",
    "        template = 'Epoch {:0}, Training loss: {:.4f}, Training accuracy: {:.4f}, Validation loss: {:.4f}, Validation Accuracy: {:.4f}'\n",
    "        tf.print (template.format(epoch+1,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               val_loss.result(),\n",
    "                               val_accuracy.result()*100))\n",
    "        \n",
    "        # Save weight after each epoch\n",
    "        ckpt_manager.save()\n",
    "        \n",
    "        # Save best weight\n",
    "        if val_accuracy.result() > best_val_acc:\n",
    "            best_val_acc = val_accuracy.result()\n",
    "            best_manager.save()\n",
    "            print(f\"Best model saved at {best_manager.latest_checkpoint}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loop(dataset_train, dataset_val, EPOCHS= HParams.NUM_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = []\n",
    "predicted_label = []\n",
    "\n",
    "def test_loop(dataset_test):\n",
    "    print(f\"Model restored from {tf.train.latest_checkpoint(Paths.BEST_CHECKPOINT_DIR)}.\")\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(Paths.BEST_CHECKPOINT_DIR))\n",
    "    \n",
    "    for image, label in tqdm(dataset_test, total=math.floor(len(img_path_test)/HParams.BATCH_SIZE)):\n",
    "        loss, labels, predictions = test_step(image, label)\n",
    "        \n",
    "        true_label.extend(labels.numpy())\n",
    "        predicted_label.extend(tf.argmax(predictions, axis=1).numpy())\n",
    "        \n",
    "        test_loss(loss)\n",
    "        test_accuracy(labels, predictions)\n",
    "\n",
    "    template = 'Test loss: {:.4f}, Test accuracy: {:.4f}'\n",
    "    tf.print(template.format(test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "test_loop(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(train_loss_list, label='Training Loss')\n",
    "plt.plot(val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_label, predicted_label)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40545c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db36eff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, datasets, layers, models\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg16\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VGG16\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdisplay\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isfinite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FiniteStatus, cy_isfinite\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _object_dtype_isnan\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_array_api.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_array_api_dispatch\u001b[39m(array_api_dispatch):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that array_api_compat is installed and NumPy version is compatible.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    array_api_compat follows NEP29, which has a higher minimum NumPy version than\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    scikit-learn.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\fixes.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreadpoolctl\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\tf\\lib\\site-packages\\scipy\\stats\\__init__.py:609\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 609\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_morestats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:982\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:925\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1423\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1395\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1522\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:142\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils, datasets, layers, models\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path('../../dataset/4500')\n",
    "\n",
    "# save (img_path, label) pairs\n",
    "train_csv_path = '../../csv/4500/train.csv'\n",
    "val_csv_path = '../../csv/4500/val.csv'\n",
    "test_csv_path = '../../csv/4500/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 300\n",
    "IMAGE_WIDTH = 300\n",
    "IMAGE_DEPTH = 3\n",
    "\n",
    "BUFFER_SIZE = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "NUM_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b96740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_root = pathlib.Path('./dataset/4500')\n",
    "\n",
    "# print the subfolders.\n",
    "print('classes:')\n",
    "for item in data_root.iterdir():\n",
    "    print(item)\n",
    "    \n",
    "all_image_paths = list(data_root.glob('*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "all_image_paths = shuffle(all_image_paths, random_state=1)\n",
    "all_image_paths = [path for path in all_image_paths if path[-3:] not in ('gif', 'bmp', 'ini')]\n",
    "image_count = len(all_image_paths)\n",
    "print('\\ntotal img num:', image_count)\n",
    "# print(f'all image path = {all_image_paths}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cc04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random showing 3 iamges \n",
    "for n in range(3):\n",
    "    image_path = random.choice(all_image_paths)\n",
    "    display.display(display.Image(image_path, width=200, height=200))\n",
    "    print(image_path.split('\\\\')[-2])\n",
    "    # print(image_path.split(os.path.sep)[-2])\n",
    "    # print(image_path.split('/')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a817f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the label\n",
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "# total label\n",
    "n_classes = len(label_names)\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mapping dict\n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "index_to_label = dict((index, name) for index,name in enumerate(label_names))\n",
    "print(label_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the label data\n",
    "all_image_label = [label_to_index[pathlib.Path(path).parent.name] for path in all_image_paths]\n",
    "print(\"First 10 label indices: \", all_image_label[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3832732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training:validation:testing sets using an 80-10-10 split\n",
    "img_path_train, img_path_val_and_test, label_train, label_val_and_test = train_test_split(all_image_paths, \n",
    "                                                                                          all_image_label,\n",
    "                                                                                          test_size=0.2,\n",
    "                                                                                          random_state=0)\n",
    "img_path_val, img_path_test, label_val, label_test = train_test_split(img_path_val_and_test, \n",
    "                                                                      label_val_and_test, \n",
    "                                                                      test_size=0.5, \n",
    "                                                                      random_state=0)\n",
    "\n",
    "\n",
    "print('training data: %d'%(len(img_path_train)))\n",
    "print('validation data: %d'%(len(img_path_val)))\n",
    "print('testing data: %d'%(len(img_path_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save (img_path, label) pairs\n",
    "with open(train_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['img_path', 'label'])\n",
    "    for img_path, label in zip(img_path_train, label_train):\n",
    "        writer.writerow([img_path, label])\n",
    "        \n",
    "with open(val_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['img_path', 'label'])\n",
    "    for img_path, label in zip(img_path_val, label_val):\n",
    "        writer.writerow([img_path, label])\n",
    "        \n",
    "with open(test_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['img_path', 'label'])\n",
    "    for img_path, label in zip(img_path_test, label_test):\n",
    "        writer.writerow([img_path, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(tf.data.Dataset):\n",
    "    OUTPUT_SIGNATURE = (\n",
    "        tf.TensorSpec(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    "\n",
    "    _INSTANCES_COUNTER = itertools.count()  # Number of datasets generated\n",
    "    _EPOCHS_COUNTER = defaultdict(itertools.count)  # Number of epochs done for each dataset\n",
    "\n",
    "    def _generator(instance_idx, filename, open_file, read_file):\n",
    "        epoch_idx = next(SimpleDataset._EPOCHS_COUNTER[instance_idx])\n",
    "\n",
    "        # Opening the file\n",
    "        img_paths, label = open_file(filename)\n",
    "\n",
    "        # Reading the file\n",
    "        for sample_idx in range(len(img_paths)):\n",
    "            img = read_file(img_paths[sample_idx])\n",
    "            yield img, label[sample_idx]\n",
    "\n",
    "    def __new__(cls, filename, open_file, read_file):\n",
    "        def generator_func(instance_idx, filename):\n",
    "            return cls._generator(instance_idx, filename, open_file, read_file)\n",
    "\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            generator_func,\n",
    "            output_signature=cls.OUTPUT_SIGNATURE,\n",
    "            args=(next(cls._INSTANCES_COUNTER), filename)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    rows = pd.read_csv(filename.decode(\"utf-8\"))\n",
    "    img_paths = rows['img_path'].tolist()\n",
    "    label = rows['label'].tolist()\n",
    "    return img_paths, label\n",
    "\n",
    "def read_file(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=IMAGE_DEPTH)\n",
    "    img = tf.image.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = tf.divide(img,255.0)\n",
    "    return img\n",
    "\n",
    "def dataset_generator_fun_train(*args):\n",
    "    return SimpleDataset(train_csv_path, open_file, read_file)\n",
    "\n",
    "def dataset_generator_fun_val(*args):\n",
    "    return SimpleDataset(val_csv_path, open_file, read_file)\n",
    "\n",
    "def dataset_generator_fun_test(*args):\n",
    "    return SimpleDataset(test_csv_path, open_file, read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse training data\n",
    "@tf.function\n",
    "def map_fun(image, label):\n",
    "    distorted_image = tf.image.random_flip_left_right(image)\n",
    "    distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image, lower=0.2, upper=1.8)\n",
    "    distorted_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "    return distorted_image, label\n",
    "\n",
    "# parse validation data\n",
    "@tf.function\n",
    "def map_fun_val(image, label):\n",
    "    distorted_image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    return distorted_image, label\n",
    "\n",
    "# parse testing data\n",
    "@tf.function\n",
    "def map_fun_test(image, label):\n",
    "    distorted_image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    return distorted_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.range(1)\\\n",
    "                               .interleave(dataset_generator_fun_train, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE, seed=SEED)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .map(map_fun, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                               .cache()\\\n",
    "                               .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "dataset_val = tf.data.Dataset.range(1)\\\n",
    "                               .interleave(dataset_generator_fun_val, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE, seed=SEED)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .map(map_fun_val, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                               .cache()\\\n",
    "                               .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "dataset_test = tf.data.Dataset.range(1)\\\n",
    "                              .interleave(dataset_generator_fun_test, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                              .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                              .map(map_fun_test, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                              .cache()\\\n",
    "                              .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in dataset_train.take(1):\n",
    "    print(img[0].shape)\n",
    "    plt.imshow(img[0]) \n",
    "    plt.axis('off') \n",
    "    plt.show()\n",
    "    print(index_to_label[label[0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3),\n",
    "    pooling=None,\n",
    ")\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "top_model = models.Sequential()\n",
    "top_model.add(layers.Flatten())\n",
    "top_model.add(layers.Dense(4096, activation='relu'))\n",
    "top_model.add(layers.Dropout(0.5))\n",
    "top_model.add(layers.Dense(1024, activation='relu'))\n",
    "top_model.add(layers.Dropout(0.5))\n",
    "top_model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "wild_model = tf.keras.Model(inputs=base_model.input, outputs=top_model(base_model.output)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0a31b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wild_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ee625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the initialization of weights \n",
    "# wild_model.save_weights('../../weight/WildModel/wild_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = wild_model(image, training=True)\n",
    "        loss = loss_object(label, predictions)\n",
    "    gradients = tape.gradient(loss, wild_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, wild_model.trainable_variables))\n",
    "    \n",
    "#     print(\"Label shape:\", label.shape)\n",
    "#     print(\"Predictions shape:\", predictions.shape)\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(label, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def val_step(image, label):\n",
    "    predictions = wild_model(image, training=False)\n",
    "    loss = loss_object(label, predictions)\n",
    "\n",
    "    val_loss(loss)\n",
    "    val_accuracy(label, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(image, label):\n",
    "    predictions = wild_model(image, training=False)\n",
    "    loss = loss_object(label, predictions)\n",
    "\n",
    "    return loss, label, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b8ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def epoch_loop(dataset_train, dataset_test, EPOCHS):\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Reset the metrics at the start of the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "\n",
    "        tf.print(\"training:\")\n",
    "        for image, label in tqdm(dataset_train, total=math.floor(len(img_path_train)/BATCH_SIZE)):\n",
    "            train_step(image, label)\n",
    "\n",
    "        tf.print(\"validating:\")\n",
    "        for image, label in tqdm(dataset_val, total=math.floor(len(img_path_val)/BATCH_SIZE)):\n",
    "            val_step(image, label)\n",
    "            \n",
    "        train_loss_list.append(train_loss.result().numpy())\n",
    "        val_loss_list.append(val_loss.result().numpy())\n",
    "\n",
    "            \n",
    "        template = 'Epoch {:0}, Training loss: {:.4f}, Training accuracy: {:.4f}, Validation loss: {:.4f}, Validation Accuracy: {:.4f}'\n",
    "        tf.print (template.format(epoch+1,\n",
    "                               train_loss.result(),\n",
    "                               train_accuracy.result()*100,\n",
    "                               val_loss.result(),\n",
    "                               val_accuracy.result()*100))\n",
    "        \n",
    "        # Save weights after each epoch\n",
    "        wild_model.save_weights('../../weight/WildModel/wild_model_epoch_{}.h5'.format(epoch + 1), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84216d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# wild_model.load_weights('../../weight/WildModel/wild_model.h5')\n",
    "epoch_loop(dataset_train, dataset_val, EPOCHS= NUM_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wild_model.load_weights(f'../../weight/WildModel/wild_model_epoch_{NUM_EPOCH+1}.h5')\n",
    "\n",
    "true_label = []\n",
    "predicted_label = []\n",
    "\n",
    "def test_loop(dataset_test):\n",
    "    for image, label in tqdm(dataset_test, total=math.floor(len(img_path_test)/BATCH_SIZE)):\n",
    "        loss, labels, predictions = test_step(image, label)\n",
    "        \n",
    "        true_label.extend(labels.numpy())\n",
    "        predicted_label.extend(tf.argmax(predictions, axis=1).numpy())\n",
    "        \n",
    "        test_loss(loss)\n",
    "        test_accuracy(labels, predictions)\n",
    "\n",
    "    template = 'Test loss: {:.4f}, Test accuracy: {:.4f}'\n",
    "    tf.print(template.format(test_loss.result(), test_accuracy.result()*100))\n",
    "\n",
    "test_loop(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(train_loss_list, label='Training Loss')\n",
    "plt.plot(val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_label, predicted_label)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40545c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
